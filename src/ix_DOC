
Index Manager for RedBase

Nandu Jayakumar
CS 346 Spring 2011
nandu@cs.stanford.edu


---------------------------------------

Overall Design:

    An index entry is composed of a key and an RID that points to the
    data location on a RM file. A B+tree implementation is used to
    keep an ordered index on the data. This B+tree is built with a <=
    assumption. Any pointer in an intermediate node points to a child
    node one level below it such that they key in the intermediate
    node is the child's largest key.

    B+tree Layout -
    Simplified to use the same, size and structure for intermediate
    and leaf nodes. Intermediate nodes do not use the SlotID part of
    the RID but the same structure was maintained for simplicity.
    Also simplified to store an (implied) key in the last location on
    each intermediate node.
    Additional left and right pointers (PageIDs) are also stored on
    each node - intermediate as well as leaf.
    The leaf level is also strictly maintained in an ordered manner so
    that an index scan can be used to read the data in a sorted way in
    either direction.

    Duplicate Handling -
    My design handles the presence of duplicate keys in each B+tree
    node. This is done by writing several methods that make a
    "right-most match" guarantee and by implementing all these methods
    so that they are aware of duplicates. I decided that this would
    provide better IO benefits than bucket page maintenance.

    Deletion Algorithm -
    A lazy deletion algorithm was used. An underflow is implemented as
    a node (intermediate or leaf) which has 0 keys left.

    Scan Optimizations - 
    The index is expected to primarily be used via the scan
    interface. Several optimizations use logarithmic B+Tree lookups to
    set-up a starting and ending point for the eventual scan so that
    the number of leaf nodes scanned is minimized based on the
    operation. As an example, OpenScan() with EQ_OP on a non-existent
    key will set GetNextEntry() up to return EOF when it is called the
    first time itself. Also, the Scan class supports both descending
    and ascending (default) scans and uses the right one based on the
    user's request or optimizations.
    State of the scan is maintained via current leaf node and
    current position in leaf node (position just scanned). Additional
    state like last node/position are also used to optimize the
    start/end points of the scan.


---------------------------------------

Key Data Structures:

    Within a B+tree(node) page the list of keys is always maintained
    in sorted order. So binary search algorithms can be used everywhere.

    There is no Page Header but each Page node also keeps track of
    left/right PageIDs as well as the number of keys and serializes
    this into the page.

    The FileHeader (RM_FileHdr) keeps track of the root page and the
    number of pages in the B+Tree. It also maintains the height of the
    tree and its order along with other treewide information like
    attrType and length.


    
---------------------------------------

Testing:

   Automated unit tests were used to test each class. A popular test
   harness - google-test, which is in the
   same style as JUnit was used to make testing fast and automatic.
   Additionally, the ix_test (modified slightly)
   was also used to test correctness of the data.
   Different page sizes were tested as well so that insertion/deletion
   could be tested with the class/paper examples in the test cases.
   The tests also contain the B+tree invariants used to ensure that a
   balanced and correct tree resulted from each operation.

---------------------------------------

Bugs/Known Issues:
    
    Marked under several places in the code with //TODO.

